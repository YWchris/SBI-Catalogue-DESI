{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the notebook for predicting the [Fe/H] in the DESI spectra using masked autoregressive flow\n",
    "\n",
    "The model is trained using labels from APOGEE\n",
    "\n",
    "The structure of this notebook follows:\n",
    "\n",
    "1. Import packages for this notebook, we use torch and the model is from the sbi package\n",
    "2. Load the data from two sources: Spectra with signal to noise ratio < 50 and Spectra with signal to noise ratio > 50\n",
    "3. Cross-match spectra between desi and apogee and normalize the spectra, Remove rows in apogee with abnormal Fe/H values\n",
    "4. Train the MAF models and save them \n",
    "5. Test the model performances: 2D histograms (Accuracy check), SBC (Posterior well calibrated or not)\n",
    "6. Application to Globular Clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tingli/.conda/envs/myenv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sbi.inference import SNPE\n",
    "from sbi import utils as utils\n",
    "from sbi.analysis import run_sbc\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table, Column\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 load spectra with lower SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file path\n",
    "f1 = \"/raid/users/heigerm/catalogues/sptab_spspectra_rvtab_lowsnr.fits\"\n",
    "HDUlist1 = fits.open(f1)\n",
    "\n",
    "# DESI labels\n",
    "sp_tab1 = Table(HDUlist1['SPTAB'].data) \n",
    "\n",
    "# APOGEE labels\n",
    "apogee_tab1 = Table(HDUlist1[4].data)\n",
    "\n",
    "# DESI SP Spectra\n",
    "spectra1 = Table(HDUlist1['SPECTRA_SP'].data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 load spectra with SNR > 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file path\n",
    "f2 = \"/raid/users/heigerm/catalogues/sp_x_apogee_x_spspectra_rvtab.fits\" \n",
    "HDUlist2 = fits.open(f2)\n",
    "\n",
    "# DESI labels\n",
    "sp_tab2 = Table(HDUlist2['SPTAB'].data)  \n",
    "\n",
    "# APOGEE labels\n",
    "apogee_tab2 = Table(HDUlist2['APOGEEDR17'].data)\n",
    "\n",
    "# DESI SP Spectra\n",
    "spectra2 = Table(HDUlist2['SPECTRA_SP'].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Cross-match DESI with APOGEE (ra, dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SkyCoord objects for both tables\n",
    "apogee_coords = SkyCoord(ra=apogee_tab1['RA']*u.degree, dec=apogee_tab1['DEC']*u.degree)\n",
    "spectra_coords = SkyCoord(ra=spectra1['TARGET_RA']*u.degree, dec=spectra1['TARGET_DEC']*u.degree) \n",
    "\n",
    "# Find the closest match for each entry in spectra1 within a tolerance\n",
    "idx, d2d, _ = spectra_coords.match_to_catalog_sky(apogee_coords)\n",
    "\n",
    "tolerance = 1 * u.arcsec\n",
    "matches_within_tolerance = d2d < tolerance\n",
    "\n",
    "apogee_tab1_matched = apogee_tab1[idx[matches_within_tolerance]]\n",
    "spectra1_matched = spectra1[matches_within_tolerance]\n",
    "sp_tab1_matched = sp_tab1[matches_within_tolerance]\n",
    "\n",
    "# stack the apogee labels from the two tables\n",
    "apogee_tab1_selected = apogee_tab1_matched['APOGEE_ID', 'RA', 'DEC', 'FE_H', 'FE_H_ERR']\n",
    "apogee_tab2_selected = apogee_tab2['APOGEE_ID', 'RA', 'DEC', 'FE_H', 'FE_H_ERR']\n",
    "\n",
    "# obtain the apogee table\n",
    "apogee_tab_combined = vstack([apogee_tab1_selected, apogee_tab2_selected])\n",
    "\n",
    "# Combine the spectra from the two tables\n",
    "# Select the common columns from each table\n",
    "common_cols = set(spectra1_matched.colnames).intersection(spectra2.colnames)\n",
    "spectra1_common = spectra1_matched[list(common_cols)]\n",
    "spectra2_common = spectra2[list(common_cols)]\n",
    "\n",
    "# obtain the spectra table\n",
    "spectra_combined = vstack([spectra1_common, spectra2_common])\n",
    "\n",
    "# Stack DESI labels from the two tables\n",
    "common_cols = set(sp_tab1_matched.colnames).intersection(sp_tab2.colnames)\n",
    "sp1_common = sp_tab1_matched[list(common_cols)]\n",
    "sp2_common = sp_tab2[list(common_cols)]\n",
    "\n",
    "# obtain the DESI table\n",
    "sp_combined = vstack([sp1_common, sp2_common])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Remove abnormal rows for [Fe/H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the rows if Fe/H is nan or larger than 10, or have zero values for Fe/H and its error\n",
    "abnormal_rows = np.where((np.isnan(apogee_tab_combined['FE_H'])) | (apogee_tab_combined['FE_H'] > 10)\n",
    "                        |(apogee_tab_combined['FE_H'] == 0) |\n",
    "                        (apogee_tab_combined['FE_H_ERR'] == 0) |\n",
    "                        (np.isnan(apogee_tab_combined['FE_H_ERR'])))[0]\n",
    "\n",
    "# Create a mask to filter out the abnormal rows\n",
    "mask = ~np.isin(np.arange(len(apogee_tab_combined)), abnormal_rows)\n",
    "\n",
    "# Apply the mask to the datasets to filter out the abnormal rows\n",
    "apogee_tab_masked = apogee_tab_combined[mask]\n",
    "spectra_masked = spectra_combined[mask]\n",
    "sp_masked = sp_combined[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Combine the spectra from three arms and normalize (x-median/iqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_combined_spectra = Table(names=['combined_flux', 'combined_wavelength'], dtype=['object', 'object'])\n",
    "\n",
    "for row in spectra_masked:\n",
    "    # Combine and sort flux and wavelength from all arms\n",
    "    combined_flux = np.concatenate([row['flx_B'], row['flx_R'], row['flx_Z']])\n",
    "    combined_wavelength = np.concatenate([row['B_WAVELENGTH'], row['R_WAVELENGTH'], row['Z_WAVELENGTH']])\n",
    "    sort_order = np.argsort(combined_wavelength)\n",
    "    combined_flux, combined_wavelength = combined_flux[sort_order], combined_wavelength[sort_order]\n",
    "\n",
    "    # Normalize flux\n",
    "    global_median = np.median(combined_flux)\n",
    "    IQR = np.percentile(combined_flux, 75) - np.percentile(combined_flux, 25)\n",
    "    normalized_flux = (combined_flux - global_median) / IQR\n",
    "\n",
    "    gb_combined_spectra.add_row([normalized_flux, combined_wavelength])\n",
    "    \n",
    "flux = np.array(gb_combined_spectra['combined_flux'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Set up Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input spectra\n",
    "X = np.array([np.array(flux_val, dtype=float) for flux_val in flux])\n",
    "\n",
    "# Parameter [Fe/H]\n",
    "theta = np.array(apogee_tab_masked[\"FE_H\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Cross Valiation Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 fold cross validation\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# define the results to be saved\n",
    "results = {'exp': [], 'pred': [], 'res': [], 'var': []}\n",
    "\n",
    "# objects for simulation-based calibration\n",
    "test_posterior_samples, sbc_ranks, sbc_dap_samples, all_x_test, all_y_test = [], [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Train and Save Models (Masked Autoregressive Flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the folds\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X, theta)):\n",
    "\n",
    "\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = theta[train_index], theta[test_index]\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train, X_test = map(torch.Tensor, (X_train, X_test))\n",
    "    y_train, y_test = map(torch.Tensor, (y_train, y_test))\n",
    "    y_train, y_test = y_train.unsqueeze(-1), y_test.unsqueeze(-1)\n",
    "    # save the test data\n",
    "    all_x_test.append(X_test)\n",
    "    all_y_test.append(y_test)\n",
    "        \n",
    "    # Masked Autoregressive Flow\n",
    "    inference = SNPE(density_estimator=\"maf\")\n",
    "    inference.append_simulations(y_train, X_train)\n",
    "    \n",
    "    # Train MAF \n",
    "    density_estimator = inference.train()\n",
    "    \n",
    "    # Obtain the posterior \n",
    "    posterior = inference.build_posterior(density_estimator)\n",
    "\n",
    "    # Save the posterior\n",
    "    model_pkl_file = f\"MAF_fold{fold}.pkl\" \n",
    "    \n",
    "    with open(model_pkl_file, 'wb') as file:\n",
    "        pickle.dump(posterior, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Obtain the predictions and Simulation-based Calibration ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the models\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X, theta)):\n",
    "    \n",
    "    model_pkl_file = f\"MAF_fold{fold}.pkl\"\n",
    "    with open(model_pkl_file, 'rb') as file:\n",
    "        posterior = pickle.load(file)\n",
    "        \n",
    "    # Sample from the posterior for the test set\n",
    "    n_samples = 250  \n",
    "    for idx in range(len(X_test)):\n",
    "        samples = posterior.sample((n_samples,), x=X_test[idx])\n",
    "        target_samples = samples[:, 0]\n",
    "        target_exp = y_test[idx]\n",
    "        target_pred = torch.mean(target_samples)\n",
    "        target_res = target_pred - target_exp\n",
    "        target_var = torch.var(target_samples)\n",
    "\n",
    "        results['exp'].append(target_exp)\n",
    "        results['pred'].append(target_pred)\n",
    "        results['res'].append(target_res)\n",
    "        results['var'].append(target_var)\n",
    "        \n",
    "    # Simulation-Based Calibration (SBC)\n",
    "    num_posterior_samples=1000\n",
    "    ranks, dap_samples = run_sbc(y_test, X_test, posterior, num_posterior_samples=num_posterior_samples, reduce_fns='marginals')\n",
    "    sbc_ranks.append(ranks)\n",
    "    sbc_dap_samples.append(dap_samples)\n",
    "    \n",
    "    \n",
    "# [Fe/H] truth from APOGEE    \n",
    "feh_exp = torch.stack(results['exp'])\n",
    "\n",
    "# [Fe/H] predictions\n",
    "feh_pred = torch.stack(results['pred']) \n",
    "\n",
    "# [Fe/H] residuals\n",
    "feh_res = torch.stack(results['res']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. 2D Histogram: DESI SP vs. APOGEE, MAF (our model predictions) vs. APOGEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = mcolors.LinearSegmentedColormap.from_list('greenblue', ['white', 'dodgerblue', 'dodgerblue', 'royalblue', 'royalblue', 'mediumblue', 'mediumblue', 'midnightblue'])\n",
    "feh_exp = np.array(feh_exp)\n",
    "feh_pred = np.array(feh_pred)\n",
    "feh_res = np.array(feh_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up plot for DESI vs. APOGEE\n",
    "xbin = np.linspace(-2, 0.5, 100) # range of Fe/H values\n",
    "lower = [] # -1 sigma\n",
    "median = []\n",
    "upper = [] # +1 sigma\n",
    "center = []\n",
    "for n in range(len(xbin)-1):\n",
    "    edges = [i for i in range(len(feh_exp)) if (feh_exp[i] >= xbin[n] and feh_exp[i] < xbin[n+1])]\n",
    "    residual = [] \n",
    "    if edges:\n",
    "        c = (xbin[n+1] + xbin[n]) / 2 \n",
    "        center.append(c)\n",
    "        for k in edges:\n",
    "            residual.append(feh_res[k]) \n",
    "        p = np.percentile(residual, (16, 50, 84)) \n",
    "        lower.append(p[0])\n",
    "        median.append(p[1])\n",
    "        upper.append(p[2])\n",
    "\n",
    "\n",
    "center = np.array(center, dtype=float)\n",
    "lower = np.array(lower, dtype=float)\n",
    "median = np.array(median, dtype=float)\n",
    "upper = np.array(upper, dtype=float)      \n",
    "        \n",
    "l = np.interp(xbin, center, lower) \n",
    "m = np.interp(xbin, center, median)\n",
    "u = np.interp(xbin, center, upper)\n",
    "\n",
    "desi_feh = np.array(sp_masked['FEH'])\n",
    "apogee_feh = np.array(apogee_tab_masked['FE_H'])\n",
    "xbin1 = np.linspace(-2, 0.5, 100)\n",
    "r1 = np.array(desi_feh) - np.array(apogee_feh) # residuals between DESI and APOGEE\n",
    "\n",
    "# set up plot for MAF (our model) vs. APOGEE\n",
    "lower1 = []\n",
    "median1 = []\n",
    "upper1 = []\n",
    "center1 = []\n",
    "for n in range(len(xbin1)-1):\n",
    "    edges = [i for i in range(len(feh_exp)) if (feh_exp[i] >= xbin1[n] and feh_exp[i]< xbin1[n+1])]\n",
    "    residual = []\n",
    "    if edges:\n",
    "        c = (xbin1[n+1] + xbin1[n])/2\n",
    "        center1.append(c)\n",
    "        for k in edges:\n",
    "            residual.append(r1[k])\n",
    "        p = np.percentile(residual, (16, 50, 84))\n",
    "        lower1.append(p[0])\n",
    "        median1.append(p[1])\n",
    "        upper1.append(p[2])\n",
    "\n",
    "\n",
    "center1 = np.array(center1, dtype=float)\n",
    "lower1 = np.array(lower1, dtype=float)\n",
    "median1 = np.array(median1, dtype=float)\n",
    "upper1 = np.array(upper1, dtype=float)         \n",
    "        \n",
    "        \n",
    "l1 = np.interp(xbin1, center1, lower1)\n",
    "m1 = np.interp(xbin1, center1, median1)\n",
    "u1 = np.interp(xbin1, center1, upper1)\n",
    "\n",
    "# Plot the 2D histograms! \n",
    "fig, (ax1,ax2) = plt.subplots(2, 1, figsize=(14,12))\n",
    "ax1 = plt.subplot(2, 1, 1)\n",
    "h1, x, y, i = plt.hist2d(feh_exp, r1, bins = (np.linspace(-2, 0.5, 100), np.linspace(-0.75, 0.75, 50)), \n",
    "                         cmap = cmap)\n",
    "plt.plot(xbin1, l1, c = 'black', linestyle = '--', label = '-1$\\sigma$', lw = 2)\n",
    "plt.plot(xbin1, m1, c = 'black', label = 'Median', lw = 2)\n",
    "plt.plot(xbin1, u1, c = 'black', linestyle = 'dotted', label = '+1$\\sigma$', lw = 2)\n",
    "\n",
    "plt.ylabel('$\\Delta$ Before', fontsize = 18)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.legend(fontsize = 16)\n",
    "ax1.tick_params(axis = 'x', labelbottom=False)\n",
    "im1 = ax1.imshow(h1, cmap = cmap)\n",
    "\n",
    "\n",
    "ax2 = plt.subplot(2, 1, 2)\n",
    "h2, x, y, i = plt.hist2d(feh_exp, feh_res, bins = (np.linspace(-2, 0.5, 100), np.linspace(-0.75, 0.75, 50)), cmap = cmap)\n",
    "plt.plot(xbin, l, c = 'black', linestyle = '--', label = '-1$\\sigma$', lw = 2)\n",
    "plt.plot(xbin, m, c = 'black', label = 'Median', lw = 2)\n",
    "plt.plot(xbin, u, c = 'black', linestyle = 'dotted', label = '+1$\\sigma$', lw = 2)\n",
    "\n",
    "plt.xlabel('APOGEE [Fe/H]', fontsize = 18)\n",
    "plt.ylabel('$\\Delta$ After', fontsize = 18)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.legend(fontsize = 18)\n",
    "im2 = ax2.imshow(h2, cmap = cmap)\n",
    "\n",
    "cbar = fig.colorbar(im1, ax=(ax1, ax2))\n",
    "cbar.ax.tick_params(labelsize = 16) \n",
    "cbar.set_label('# of stars', fontsize = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Simulation-based Calibration Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbc_x_test = torch.cat(all_x_test, dim = 0)\n",
    "sbc_x_test = sbc_x_test.numpy()\n",
    "\n",
    "sbc_y_test = torch.cat(all_y_test, dim = 0)\n",
    "sbc_y_test = sbc_y_test.numpy()\n",
    "\n",
    "sbc_ranks_test = torch.cat(sbc_ranks, dim = 0)\n",
    "sbc_ranks_test = sbc_ranks_test.numpy()\n",
    "\n",
    "sbc_dap_samples_test = torch.cat(sbc_dap_samples, dim = 0)\n",
    "sbc_dap_samples_test = sbc_dap_samples_test.numpy()\n",
    "\n",
    "sbc_ranks_test_tensor = torch.tensor(sbc_ranks_test)\n",
    "sbc_dap_samples_tensor = torch.tensor(sbc_dap_samples_test)\n",
    "sbc_y_test_tensor = torch.tensor(sbc_y_test)\n",
    "\n",
    "# SBC Rank Plot\n",
    "f, ax = sbc_rank_plot(\n",
    "    ranks=ranks,\n",
    "    num_posterior_samples=num_posterior_samples,\n",
    "    parameter_labels = ['Fe/H'],\n",
    "    plot_type=\"hist\",\n",
    "    num_bins=None)\n",
    "\n",
    "# SBC CDF Plot\n",
    "f, ax = sbc_rank_plot(ranks, 1_000, parameter_labels = ['Fe/H'], \n",
    "                      plot_type=\"cdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Application to Globular Clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Load Globular Cluster Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "# Globular Clusters with 4 truths of [Fe/H]\n",
    "gc_filename1 = \"/home/jupyter-tingli/DESI/gc_iron_240123.fits\"\n",
    "gc_HDUlist1 = fits.open(gc_filename1)\n",
    "\n",
    "# Globular Cluster Spectra\n",
    "gc_spectra1 = Table(gc_HDUlist1[1].data)\n",
    "\n",
    "# print the globular clusters in gc_spectra\n",
    "set(gc_spectra1['gcname'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Extract true [Fe/H] from the literature, each GC has 4 truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_gcnames = set(gc_spectra1['gcname'])\n",
    "\n",
    "# Dictionary to hold the metallicity values for each gcname\n",
    "gc_metallicities = {}\n",
    "\n",
    "for gcname in unique_gcnames:\n",
    "    # Filter the table for the current gcname\n",
    "    gc_data = gc_spectra1[gc_spectra1['gcname'] == gcname]\n",
    "    \n",
    "    # Assuming there's at least one entry for each gcname and the values are consistent\n",
    "    # Extract the metallicity values for the first occurrence\n",
    "    metallicity_H10 = gc_data['metallicity_H10'][0]\n",
    "    metallicity_K19 = gc_data['metallicity_K19'][0]\n",
    "    metallicity_B19 = gc_data['metallicity_B19'][0]\n",
    "    metallicity_V20 = gc_data['metallicity_V20'][0]\n",
    "    \n",
    "    # Store the extracted values in the dictionary\n",
    "    gc_metallicities[gcname] = {\n",
    "        'metallicity_H10': metallicity_H10,\n",
    "        'metallicity_K19': metallicity_K19,\n",
    "        'metallicity_B19': metallicity_B19,\n",
    "        'metallicity_V20': metallicity_V20\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Create the spectra table for each globular cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_names = [\n",
    "    'NGC_2419', 'NGC_5024_M_53', 'NGC_5053', 'NGC_5272_M_3', 'NGC_5466',\n",
    "    'NGC_5634', 'NGC_5904_M_5', 'NGC_6205_M_13', 'NGC_6218_M_12', 'NGC_6229',\n",
    "    'NGC_6341_M_92', 'NGC_7078_M_15', 'NGC_7089_M_2', 'Pal_14', 'Pal_5'\n",
    "]\n",
    "\n",
    "gc_tables = {name: gc_spectra[gc_spectra['gcname'] == name] for name in gc_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.1. Preprocess the GC spectra (normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gc_spectra(gc_table):\n",
    "    processed_spectra = Table()\n",
    "    processed_spectra.add_column(Column(name='combined_flux', dtype='object'))\n",
    "    processed_spectra.add_column(Column(name='combined_wavelength', dtype='object'))\n",
    "\n",
    "    for row in gc_table:\n",
    "        flux_B, wavelength_B = row['flx_B'], row['B_WAVELENGTH']\n",
    "        flux_R, wavelength_R = row['flx_R'], row['R_WAVELENGTH']\n",
    "        flux_Z, wavelength_Z = row['flx_Z'], row['Z_WAVELENGTH']\n",
    "\n",
    "        combined_flux = np.concatenate([flux_B, flux_R, flux_Z])\n",
    "        combined_wavelength = np.concatenate([wavelength_B, wavelength_R, wavelength_Z])\n",
    "\n",
    "        sort_order = np.argsort(combined_wavelength)\n",
    "        combined_flux = combined_flux[sort_order]\n",
    "        combined_wavelength = combined_wavelength[sort_order]\n",
    "\n",
    "        global_median = np.median(combined_flux)\n",
    "        combined_flux -= global_median\n",
    "        IQR = np.percentile(combined_flux, 75) - np.percentile(combined_flux, 25)\n",
    "        combined_flux = combined_flux / IQR\n",
    "\n",
    "        processed_spectra.add_row([combined_flux, combined_wavelength])\n",
    "    \n",
    "    return processed_spectra\n",
    "\n",
    "gc_processed_spectra = {name: process_gc_spectra(table) for name, table in gc_tables.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. Obtain the [Fe/H] predictions for the GCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 5\n",
    "\n",
    "# store the predictions and variance\n",
    "feh_pred_gc = {gc_name: [] for gc_name in X_gc.keys()}\n",
    "feh_var_gc = {gc_name: [] for gc_name in X_gc.keys()}\n",
    "\n",
    "for fold in range(num_folds):\n",
    "    \n",
    "    # load the models\n",
    "    model_pkl_file = f\"MAF_fold{fold}.pkl\"\n",
    "    with open(model_pkl_file, 'rb') as file:\n",
    "        posterior = pickle.load(file)\n",
    "    \n",
    "    # For each globular cluster, make predictions using the loaded model\n",
    "    for gc_name, gc_data in X_gc.items():\n",
    "        # Initialize lists to store predictions and variances for the current GC\n",
    "        feh_pred_gc[gc_name] = []\n",
    "        feh_var_gc[gc_name] = []\n",
    "    \n",
    "        # Iterate through each observation in the GC data\n",
    "        for observation in gc_data:\n",
    "            # Convert the single observation to a PyTorch tensor and add an extra dimension to match input shape\n",
    "            gc_data_tensor = torch.Tensor(observation).unsqueeze(0)\n",
    "        \n",
    "            # Make predictions for the single observation\n",
    "            samples = posterior.sample((250,), x=gc_data_tensor)  # Adjust the number of samples as needed\n",
    "            fe_h = samples[:, 0]  # Extract [Fe/H] predictions\n",
    "        \n",
    "            # Calculate mean and variance of [Fe/H] predictions for the single observation\n",
    "            feh_pred_gc[gc_name].append(torch.mean(fe_h).item())\n",
    "            feh_var_gc[gc_name].append(torch.var(fe_h).item())\n",
    "\n",
    "# Calculate the overall mean and variance of [Fe/H] predictions for each GC across all folds\n",
    "feh_pred_gc_mean = {gc_name: np.mean(values) for gc_name, values in feh_pred_gc.items()}\n",
    "feh_var_gc_mean = {gc_name: np.mean(values) for gc_name, values in feh_var_gc.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5. Histograms: Fe/H predictions (with truths) for each Globular Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gc_name, predictions in feh_pred_gc.items():\n",
    "    # convert the predictions to array\n",
    "    flat_predictions = np.array(predictions)\n",
    "    \n",
    "    if flat_predictions.size > 1:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.hist(flat_predictions, bins=30, density=True, alpha=0.6, color = 'purple', label=f'Histogram for {gc_name}')\n",
    "        # load the 4 truths for each GC\n",
    "        metallicity_measures = ['metallicity_H10', 'metallicity_K19', 'metallicity_B19', 'metallicity_V20']\n",
    "        for measure in metallicity_measures:\n",
    "            if gc_name in gc_metallicities and measure in gc_metallicities[gc_name]:\n",
    "                true_value = gc_metallicities[gc_name][measure]\n",
    "                if not np.isnan(true_value):\n",
    "                    plt.axvline(x=true_value, linestyle='--', label=f'True {measure} for {gc_name}')\n",
    "        \n",
    "        plt.title(f'Histogram of [Fe/H] Predictions for {gc_name}')\n",
    "        plt.xlabel('[Fe/H] Predictions')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        print(f\"Skipping {gc_name} due to insufficient data for histogram.\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e3c0d2cb689c9f3ddfdcc20370a54f5a0d1f4658107fa1312f8e0c21d7f27d67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
